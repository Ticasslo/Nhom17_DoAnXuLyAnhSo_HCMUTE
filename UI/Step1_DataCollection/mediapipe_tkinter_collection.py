import os
import re
import time
import cv2
import warnings
import threading
from queue import Queue, Empty, Full
import numpy as np
from PIL import Image, ImageTk
import tkinter as tk
from tkinter import filedialog

import mediapipe as mp
from mediapipe.tasks.python import vision

# Gi·∫£m warning log
warnings.filterwarnings("ignore", category=UserWarning)

# ========== 1. CONFIGURATION ==========
# Camera settings
SOURCE = 0  # 0 = webcam m·∫∑c ƒë·ªãnh

# Performance settings
NUM_HANDS = 1  # 2 ng∆∞·ªùi (m·ªói ng∆∞·ªùi 2 tay) - c√≥ th·ªÉ gi·∫£m xu·ªëng 2 n·∫øu ch·ªâ c·∫ßn 1 ng∆∞·ªùi
MIN_DETECTION_CONFIDENCE = 0.6  # Ng∆∞·ª°ng cho palm detector (BlazePalm)
MIN_PRESENCE_CONFIDENCE = 0.5   # Ng∆∞·ª°ng ƒë·ªÉ trigger re-detection (th·∫•p h∆°n = re-detect th∆∞·ªùng xuy√™n h∆°n)
MIN_TRACKING_CONFIDENCE = 0.5   # Ng∆∞·ª°ng cho hand tracking (landmark model)

# Filtering thresholds
HAND_MIN_AREA_RATIO = 0.0025   # ~0.25% di·ªán t√≠ch frame (b·ªè box qu√° nh·ªè)
HAND_MAX_AREA_RATIO = 0.35     # ~35% di·ªán t√≠ch frame (b·ªè box qu√° l·ªõn)
HANDEDNESS_SCORE_THRESHOLD = 0.6  # Ng∆∞·ª°ng confidence t·ªëi thi·ªÉu cho handedness

# Display settings
PRINT_EVERY_N_FRAMES = 200
WINDOW_WIDTH = 1080
WINDOW_HEIGHT = 660

# EMA Smoothing settings
ENABLE_EMA_SMOOTHING = True  # Enable Exponential Moving Average smoothing
EMA_ALPHA = 0.5  # Smoothing factor (0.1=max smooth, 1.0=no smooth).

# Queue settings
FRAME_BUFFER_SIZE = 1
DETECTION_BUFFER_SIZE = 1

DETECTION_SKIP_FRAMES = 1  # S·ªë frame b·ªè qua gi·ªØa c√°c l·∫ßn detection (0 = detect m·ªçi frame)
# =======================================

# ---------- 2. MediaPipe Hand Landmarker ----------
script_dir = os.path.dirname(os.path.abspath(__file__))

# G·ªëc project: .../Nhom17_DoAnXuLyAnhSo_HCMUTE
project_root = os.path.dirname(os.path.dirname(script_dir))

# Model MediaPipe (.task) d√πng chung cho TO√ÄN project, ƒë·∫∑t t·∫°i: Nhom17_DoAnXuLyAnhSo_HCMUTE/models/hand_landmarker.task
HAND_LANDMARKER_MODEL_PATH = os.path.join(project_root, "models", "hand_landmarker.task")

if not os.path.exists(HAND_LANDMARKER_MODEL_PATH):
    raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y model MediaPipe: {HAND_LANDMARKER_MODEL_PATH}")

BaseOptions = mp.tasks.BaseOptions
HandLandmarker = vision.HandLandmarker
HandLandmarkerOptions = vision.HandLandmarkerOptions
VisionRunningMode = mp.tasks.vision.RunningMode

# T·ªëi ∆∞u hi·ªáu su·∫•t cho Windows:
# - L∆∞u √Ω: MediaPipe Python tr√™n Windows KH√îNG h·ªó tr·ª£ GPU delegate
# - C√°c t·ªëi ∆∞u ƒë√£ √°p d·ª•ng:
#   1. Warm-up model (gi·∫£m latency spike)
#   2. T·ªëi ∆∞u s·ªë l∆∞·ª£ng hands detect (gi·∫£m num_hands n·∫øu kh√¥ng c·∫ßn nhi·ªÅu)
#   3. Multi-threading
#   4. T·ªëi ∆∞u confidence thresholds
base_options = BaseOptions(model_asset_path=HAND_LANDMARKER_MODEL_PATH)

# MediaPipe s·ª≠ d·ª•ng 2-stage pipeline: BlazePalm (palm detector) + Hand landmark model
# Palm detector ch·ªâ ch·∫°y khi c·∫ßn (khi hand presence confidence th·∫•p), kh√¥ng ph·∫£i m·ªói frame
# ‚Üí Gi√∫p t·ªëi ∆∞u performance (theo Google Research blog)
options = HandLandmarkerOptions(
    base_options=base_options,
    running_mode=VisionRunningMode.VIDEO,  # d√πng VIDEO mode cho webcam
    num_hands=NUM_HANDS,
    min_hand_detection_confidence=MIN_DETECTION_CONFIDENCE,
    min_hand_presence_confidence=MIN_PRESENCE_CONFIDENCE,
    min_tracking_confidence=MIN_TRACKING_CONFIDENCE,
)
landmarker = HandLandmarker.create_from_options(options)

# Warm-up: ch·∫°y inference ƒë·∫ßu ti√™n ƒë·ªÉ kh·ªüi t·∫°o model (gi·∫£m latency spike khi b·∫Øt ƒë·∫ßu)
print("  ‚Üí Warming up MediaPipe model...")
try:
    dummy_frame = np.zeros((480, 640, 3), dtype=np.uint8)
    dummy_frame.flags.writeable = False  # MediaPipe kh√¥ng c·∫ßn modify image
    dummy_mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=dummy_frame)
    landmarker.detect_for_video(dummy_mp_image, 0)
    print("  ‚Üí Warm-up completed!")
except Exception as e:
    print(f"  ‚Üí Warm-up failed (non-critical): {e}")

# ---------- EMA Smoothing State ----------
# EMA (Exponential Moving Average) state for each hand
# Structure: {hand_idx: {'landmarks': array, 'last_seen': timestamp}}
ema_state = {}

def apply_ema_smoothing(hand_idx, current_landmarks, alpha=EMA_ALPHA):
    """
    Apply Exponential Moving Average smoothing to landmarks
    
    EMA formula: smoothed_t = alpha * current + (1 - alpha) * smoothed_t-1
    
    Benefits:
    - Memory efficient: Only stores 1 previous value (vs N frames for moving average)
    - Computation efficient: Only 1 multiplication + 1 addition per keypoint
    - Adaptive: Automatically adjusts to motion speed
    - Lower latency: ~16-20ms lag vs ~33-50ms for moving average
    
    Args:
        hand_idx: Hand index (for tracking across frames)
        current_landmarks: Current frame landmarks (21, 3) numpy array
        alpha: Smoothing factor (0.0=max smooth, 1.0=no smooth)
               Recommended: 0.1 (very smooth), 0.3 (balanced), 0.5 (responsive)
    
    Returns:
        smoothed_landmarks: EMA-smoothed landmarks (21, 3) numpy array
    """
    if not ENABLE_EMA_SMOOTHING:
        return current_landmarks
    
    current_time = time.time()
    
    if hand_idx not in ema_state:
        # First time seeing this hand ‚Üí initialize with current landmarks
        ema_state[hand_idx] = {
            'landmarks': current_landmarks.copy(),
            'last_seen': current_time
        }
        return current_landmarks
    
    # Apply EMA: smoothed = alpha * current + (1-alpha) * previous_smoothed
    prev_landmarks = ema_state[hand_idx]['landmarks']
    smoothed = alpha * current_landmarks + (1 - alpha) * prev_landmarks
    
    # Update state for next frame
    ema_state[hand_idx] = {
        'landmarks': smoothed,
        'last_seen': current_time
    }
    
    return smoothed

def cleanup_old_ema_state(current_hand_indices, max_age_seconds=5):
    """
    Remove EMA state for hands that haven't been seen recently
    Call this periodically to avoid memory leak
    
    Args:
        current_hand_indices: Set of hand indices detected in current frame
        max_age_seconds: Remove hands not seen for this many seconds
    """
    global ema_state
    current_time = time.time()
    
    # Remove hands not in current frame AND not seen for >max_age_seconds
    ema_state = {
        idx: state for idx, state in ema_state.items() 
        if idx in current_hand_indices or (current_time - state['last_seen']) < max_age_seconds
    }

# ---------- 3. Queue & threading setup ----------
stream_url = SOURCE
target_fps = 30.0

print("=" * 60)
print("CAMERA MODE - MediaPipe Hand Landmarker (keypoints + bbox)")

# T·ªëi ∆∞u: D√πng MSMF backend tr√™n Windows
try:
    temp_cap = cv2.VideoCapture(stream_url, cv2.CAP_MSMF)
except Exception:
    temp_cap = cv2.VideoCapture(stream_url)

if temp_cap.isOpened():
    detected_fps = temp_cap.get(cv2.CAP_PROP_FPS)
    temp_cap.release()
    if detected_fps and detected_fps > 1 and detected_fps < 240:
        target_fps = float(detected_fps)
        print(f"Detected camera FPS: {target_fps:.1f}")
    else:
        print("Detected camera FPS invalid (<=1 or >240). Using 30 FPS fallback.")
else:
    print("Warning: Unable to open camera for FPS detection. Using 30 FPS fallback.")

print(f"Source: {stream_url}")
print(f"Target FPS: {target_fps:.1f}")

total_start = time.time()

print("=" * 60)
print("MULTITHREADING MODE - MediaPipe Hand Landmarker")
print("  Thread 1: Frame Grabber (ƒë·ªçc frames t·ª´ camera)")
print("  Thread 2: Hand Landmarker (detect keypoints + bbox)")
print("  Main Thread: Display (hi·ªÉn th·ªã k·∫øt qu·∫£)")
print(f"  Frame buffer size: {FRAME_BUFFER_SIZE}")
print(f"  Detection buffer size: {DETECTION_BUFFER_SIZE}")
print("=" * 60)

frame_queue = Queue(maxsize=FRAME_BUFFER_SIZE)
display_frame_queue = Queue(maxsize=FRAME_BUFFER_SIZE)
detection_queue = Queue(maxsize=DETECTION_BUFFER_SIZE)

stop_flag = threading.Event()
queue_drop_count = 0
queue_drop_lock = threading.Lock()

def frame_grabber_thread():
    """
    Thread 1: ƒê·ªçc frame t·ª´ camera v√† ƒë∆∞a v√†o queue.
    
    T·ªëi ∆∞u: D√πng MSMF backend tr√™n Windows (nhanh h∆°n DirectShow).
    Fallback v·ªÅ default n·∫øu kh√¥ng support.
    """
    global queue_drop_count
    try:
        cap = cv2.VideoCapture(stream_url, cv2.CAP_MSMF)  # Windows: MSMF backend
    except Exception:
        cap = cv2.VideoCapture(stream_url)  # Fallback
    
    if not cap.isOpened():
        print("‚úó Error: Cannot open video source")
        stop_flag.set()
        return
    
    # T·ªëi ∆∞u OpenCV settings
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # Gi·∫£m buffer ƒë·ªÉ gi·∫£m latency
    cap.set(cv2.CAP_PROP_FPS, target_fps)  # Set FPS n·∫øu camera support
    
    frame_id = 0
    while not stop_flag.is_set():
        ret, frame = cap.read()
        if not ret:
            print("‚úó End of stream or error reading frame")
            break
        
        frame_id += 1
        frame_time = time.time()
        
        frame_for_display = frame.copy()
        
        try:
            frame_queue.put((frame_id, frame, frame_time), timeout=0.01)
        except Full:
            with queue_drop_lock:
                queue_drop_count += 1
        
        try:
            display_frame_queue.put((frame_id, frame_for_display, frame_time), timeout=0.01)
        except Full:
            pass
    
    try:
        cap.release()
    except Exception:
        pass
    stop_flag.set()
    print("Thread 1 (Frame Grabber) stopped")


def hand_landmarker_thread():
    """
    Thread 2: L·∫•y frame t·ª´ queue, ch·∫°y MediaPipe Hand Landmarker (VIDEO mode)
    v√† ƒë·∫©y k·∫øt qu·∫£ (keypoints + handedness) sang detection_queue.
    
    MediaPipe y√™u c·∫ßu RGB format v√† Image wrapper.
    T·ªëi ∆∞u: Set flags.writeable = False ƒë·ªÉ tƒÉng t·ªëc (MediaPipe kh√¥ng modify image).
    """
    global queue_drop_count, is_paused
    
    print("  ‚Üí HandLandmarker thread: MediaPipe Hand Landmarker (VIDEO mode)")
    
    frame_counter = 0
    
    while not stop_flag.is_set():
        # Check pause ƒë·ªÉ gi·∫£m CPU khi pause
        if is_paused:
            time.sleep(0.1)
            continue
        
        try:
            frame_id, frame, frame_time = frame_queue.get(timeout=0.1)
            
            frame_counter += 1
            should_skip = DETECTION_SKIP_FRAMES > 0 and frame_counter % (DETECTION_SKIP_FRAMES + 1) != 0
            
            try:
                if should_skip:
                    # Skip frame nh∆∞ng v·∫´n c·∫ßn task_done() ·ªü finally
                    continue
                # Convert BGR (OpenCV) sang RGB (MediaPipe y√™u c·∫ßu)
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                rgb_frame.flags.writeable = False  # MediaPipe kh√¥ng c·∫ßn modify image
                mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)

                ts_ms = int(frame_time * 1000)
                t0 = time.time()
                result = landmarker.detect_for_video(mp_image, ts_ms)
                t1 = time.time()
                
                inference_time = t1 - t0
                payload = (frame_id, result, inference_time, t1)

                try:
                    detection_queue.put(payload, timeout=0.01)
                except Full:
                    with queue_drop_lock:
                        queue_drop_count += 1
            except Exception as e:
                print(f"‚úó Error in HandLandmarker thread processing: {e}")
            finally:
                # ƒê·∫£m b·∫£o task_done() ch·ªâ ƒë∆∞·ª£c g·ªçi 1 l·∫ßn cho m·ªói frame
                frame_queue.task_done()
            
        except Empty:
            if stop_flag.is_set():
                break
            continue
        except Exception as e:
            print(f"‚úó Error in HandLandmarker thread (queue get): {e}")
            continue
    
    print("Thread 2 (HandLandmarker) stopped")


stream_url_str = str(stream_url)
print(f"Starting MediaPipe Hand Landmarker with source: {stream_url_str[:80]}{'...' if len(stream_url_str) > 80 else ''}")

thread1 = threading.Thread(target=frame_grabber_thread, daemon=True)
thread2 = threading.Thread(target=hand_landmarker_thread, daemon=True)

thread1.start()
time.sleep(0.5)
thread2.start()

pred_start = time.time()

# ---------- 4. Hi·ªÉn th·ªã real-time ----------
total_objects = 0
frame_count = 0
MAX_FPS_HISTORY = 300
fps_list = []
frame_intervals = []
display_latencies = []
inference_fps_list = []
inference_times = []
input_fps_list = []

prev_display_time = time.time()
prev_capture_time = None

# Thread-safe shared state: latest_detection ƒë∆∞·ª£c kh·ªüi t·∫°o None ·ªü global scope
# T·∫•t c·∫£ truy c·∫≠p ƒë·ªÅu ƒë∆∞·ª£c b·∫£o v·ªá b·∫±ng latest_detection_lock ƒë·ªÉ tr√°nh race condition
latest_detection = None
latest_detection_lock = threading.Lock()

# Thread-safe shared state: current_frame ƒë·ªÉ ch·ª•p ·∫£nh ng·∫´u nhi√™n
current_frame = None
current_frame_lock = threading.Lock()

# Cache container size ƒë·ªÉ tr√°nh g·ªçi winfo_width/height m·ªói frame (performance)
cached_container_size = {'w': WINDOW_WIDTH, 'h': WINDOW_HEIGHT, 'last_scale': 1.0, 'last_w': 0, 'last_h': 0}
cached_metrics_values = {}  # Cache metrics values ƒë·ªÉ ch·ªâ update khi thay ƒë·ªïi

# UI State
is_paused = False

# Auto-save hand images settings
SAVE_HAND_IMAGES = False  # B·∫≠t/t·∫Øt t·ª± ƒë·ªông l∆∞u ·∫£nh
SAVE_DIR = os.path.join(script_dir, "dataset", "S_Test")  # Th∆∞ m·ª•c l∆∞u ·∫£nh
save_image_counter = 0  # ƒê·∫øm s·ªë ·∫£nh ƒë√£ l∆∞u
last_save_time = 0  # Th·ªùi gian l∆∞u ·∫£nh cu·ªëi c√πng (ƒë·ªÉ tr√°nh l∆∞u qu√° nhi·ªÅu)
SAVE_INTERVAL = 0.5  # Kho·∫£ng th·ªùi gian t·ªëi thi·ªÉu gi·ªØa c√°c l·∫ßn l∆∞u (gi√¢y)
notification_timer = None  # Timer ƒë·ªÉ ·∫©n th√¥ng b√°o
notification_label = None  # Label hi·ªÉn th·ªã th√¥ng b√°o (s·∫Ω ƒë∆∞·ª£c kh·ªüi t·∫°o trong UI)
root = None  # Tkinter root window (s·∫Ω ƒë∆∞·ª£c kh·ªüi t·∫°o trong UI)

# Continuous capture state
auto_capture_enabled = False
auto_capture_job = None

def scan_missing_image_numbers(save_dir):
    """
    Qu√©t th∆∞ m·ª•c ƒë·ªÉ t√¨m c√°c s·ªë c√≤n thi·∫øu trong kho·∫£ng t·ª´ 0 ƒë·∫øn s·ªë l∆∞·ª£ng file hi·ªán c√≥
    ∆Øu ti√™n l·∫•p v√†o c√°c s·ªë c√≤n thi·∫øu tr∆∞·ªõc khi ti·∫øp t·ª•c l∆∞u c√°c s·ªë ti·∫øp theo
    
    Args:
        save_dir: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c l∆∞u ·∫£nh
    
    Returns:
        tuple: (missing_numbers: list c√°c s·ªë c√≤n thi·∫øu ƒë√£ s·∫Øp x·∫øp, next_counter: s·ªë ti·∫øp theo ƒë·ªÉ l∆∞u)
    """
    if not os.path.exists(save_dir):
        return [], 0
    
    try:
        # L·∫•y danh s√°ch t·∫•t c·∫£ c√°c file trong th∆∞ m·ª•c
        files = os.listdir(save_dir)
        
        # T√¨m t·∫•t c·∫£ c√°c s·ªë counter t·ª´ c√°c file c√≥ format: NUMBER.jpg
        pattern = re.compile(r"^(\d+)\.jpg$", re.IGNORECASE)
        existing_numbers = set()
        max_counter = -1
        
        for filename in files:
            match = pattern.match(filename)
            if match:
                counter = int(match.group(1))
                existing_numbers.add(counter)
                if counter > max_counter:
                    max_counter = counter
        
        # N·∫øu kh√¥ng c√≥ file n√†o, tr·∫£ v·ªÅ list r·ªóng v√† b·∫Øt ƒë·∫ßu t·ª´ 0
        if max_counter < 0:
            return [], 0
        
        # T√¨m c√°c s·ªë c√≤n thi·∫øu trong kho·∫£ng t·ª´ 0 ƒë·∫øn max_counter
        missing_numbers = []
        for i in range(max_counter + 1):
            if i not in existing_numbers:
                missing_numbers.append(i)
        
        # S·∫Øp x·∫øp c√°c s·ªë c√≤n thi·∫øu ƒë·ªÉ d√πng theo th·ª© t·ª±
        missing_numbers.sort()
        
        # S·ªë ti·∫øp theo ƒë·ªÉ l∆∞u l√† max_counter + 1
        next_counter = max_counter + 1
        
        return missing_numbers, next_counter
    except Exception as e:
        print(f"‚ö† L·ªói khi qu√©t th∆∞ m·ª•c l∆∞u ·∫£nh: {e}")
        return [], 0

# Kh·ªüi t·∫°o: qu√©t th∆∞ m·ª•c v√† l∆∞u c√°c s·ªë c√≤n thi·∫øu
missing_image_numbers = []  # Queue c√°c s·ªë c√≤n thi·∫øu c·∫ßn l·∫•p v√†o
save_image_counter = 0  # S·ªë ti·∫øp theo ƒë·ªÉ l∆∞u (sau khi ƒë√£ l·∫•p h·∫øt s·ªë c√≤n thi·∫øu)

# Qu√©t th∆∞ m·ª•c khi kh·ªüi ƒë·ªông
missing_image_numbers, save_image_counter = scan_missing_image_numbers(SAVE_DIR)
if missing_image_numbers:
    print(f"‚úì ƒê√£ t√¨m th·∫•y {len(missing_image_numbers)} s·ªë c√≤n thi·∫øu: {missing_image_numbers}")
    print(f"  S·∫Ω ∆∞u ti√™n l·∫•p v√†o c√°c s·ªë n√†y tr∆∞·ªõc khi ti·∫øp t·ª•c t·ª´ s·ªë {save_image_counter}")
elif save_image_counter > 0:
    print(f"‚úì ƒê√£ t√¨m th·∫•y {save_image_counter} ·∫£nh trong th∆∞ m·ª•c. B·∫Øt ƒë·∫ßu t·ª´ s·ªë {save_image_counter}")

def capture_random_image(silent=False):
    """
    Ch·ª•p m·ªôt ·∫£nh ng·∫´u nhi√™n 640x640 t·ª´ frame hi·ªán t·∫°i
    
    Args:
        silent: N·∫øu True, kh√¥ng in debug messages (d√πng cho ch·ª•p li√™n t·ª•c)
    """
    global save_image_counter, last_save_time, notification_label, notification_timer, root
    global missing_image_numbers, current_frame, current_frame_lock, SAVE_HAND_IMAGES
    
    if not silent:
        print("üîç B·∫Øt ƒë·∫ßu ch·ª•p ·∫£nh ng·∫´u nhi√™n...")
    
    # C·∫£nh b√°o n·∫øu auto-save ch∆∞a b·∫≠t nh∆∞ng v·∫´n cho ph√©p ch·ª•p
    if not SAVE_HAND_IMAGES and not silent:
        print("‚ö† Auto-save ch∆∞a b·∫≠t, nh∆∞ng v·∫´n cho ph√©p ch·ª•p ·∫£nh ng·∫´u nhi√™n")
    
    # Ki·ªÉm tra kho·∫£ng th·ªùi gian gi·ªØa c√°c l·∫ßn l∆∞u
    current_time = time.time()
    if current_time - last_save_time < SAVE_INTERVAL:
        print(f"‚ö† Vui l√≤ng ƒë·ª£i {SAVE_INTERVAL:.1f}s gi·ªØa c√°c l·∫ßn ch·ª•p")
        return
    
    # L·∫•y frame hi·ªán t·∫°i (thread-safe)
    frame = None
    with current_frame_lock:
        if current_frame is not None:
            try:
                frame = current_frame.copy()
                print(f"‚úì ƒê√£ l·∫•y frame: {frame.shape if frame is not None else 'None'}")
            except Exception as e:
                print(f"‚úó L·ªói khi copy frame: {e}")
        else:
            print("‚ö† current_frame l√† None - ch∆∞a c√≥ frame n√†o ƒë∆∞·ª£c l∆∞u")
    
    if frame is None:
        print("‚ö† Ch∆∞a c√≥ frame ƒë·ªÉ ch·ª•p - vui l√≤ng ƒë·ª£i camera kh·ªüi ƒë·ªông")
        if notification_label:
            notification_label.config(text="‚ö† Ch∆∞a c√≥ frame!", fg='#ffa500')
            if notification_timer:
                root.after_cancel(notification_timer)
            def restore_status():
                if notification_label:
                    if SAVE_HAND_IMAGES:
                        notification_label.config(text="‚úì Auto-save: ON", fg='#00ff00')
                    else:
                        notification_label.config(text="‚úó Auto-save: OFF", fg='#ff6b6b')
            notification_timer = root.after(2000, restore_status)
        return
    
    try:
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
        os.makedirs(SAVE_DIR, exist_ok=True)
        
        frame_h, frame_w = frame.shape[:2]
        target_size = 640
        is_resized = False  # Flag ƒë·ªÉ theo d√µi xem c√≥ resize kh√¥ng
        
        # X·ª≠ l√Ω frame: resize ho·∫∑c c·∫Øt ng·∫´u nhi√™n t√πy k√≠ch th∆∞·ªõc
        if frame_w < target_size or frame_h < target_size:
            # Frame nh·ªè h∆°n 640x640: resize to√†n b·ªô frame l√™n 640x640 (gi·ªØ t·ª∑ l·ªá v√† pad v·ªõi m√†u ƒëen)
            print(f"‚úì Frame nh·ªè ({frame_w}x{frame_h}), t·ª± ƒë·ªông resize l√™n {target_size}x{target_size}")
            is_resized = True
            
            # T√≠nh scale ƒë·ªÉ fit v√†o 640x640 (gi·ªØ t·ª∑ l·ªá)
            scale = min(target_size / frame_w, target_size / frame_h)
            new_w = int(frame_w * scale)
            new_h = int(frame_h * scale)
            
            # Resize frame
            frame_resized = cv2.resize(frame, (new_w, new_h), interpolation=cv2.INTER_AREA)
            
            # T·∫°o ·∫£nh 640x640 v·ªõi background ƒëen
            random_crop = np.zeros((target_size, target_size, 3), dtype=np.uint8)
            
            # ƒê·∫∑t ·∫£nh ƒë√£ resize v√†o gi·ªØa
            y_offset = (target_size - new_h) // 2
            x_offset = (target_size - new_w) // 2
            random_crop[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = frame_resized
            
            random_x = x_offset
            random_y = y_offset
        else:
            # Frame ƒë·ªß l·ªõn: c·∫Øt ng·∫´u nhi√™n m·ªôt v√πng 640x640
            # Ch·ªçn v·ªã tr√≠ ng·∫´u nhi√™n ƒë·ªÉ c·∫Øt ·∫£nh 640x640
            max_x = frame_w - target_size
            max_y = frame_h - target_size
            
            # ƒê·∫£m b·∫£o max_x v√† max_y >= 0
            if max_x < 0:
                max_x = 0
            if max_y < 0:
                max_y = 0
            
            # Random v·ªã tr√≠ c·∫Øt
            random_x = np.random.randint(0, max_x + 1) if max_x > 0 else 0
            random_y = np.random.randint(0, max_y + 1) if max_y > 0 else 0
            
            # C·∫Øt ·∫£nh 640x640
            random_crop = frame[random_y:random_y+target_size, random_x:random_x+target_size]
        
        if random_crop.size == 0:
            print("‚ö† Kh√¥ng th·ªÉ x·ª≠ l√Ω ·∫£nh")
            return
        
        # T√¨m s·ªë counter ƒë·ªÉ l∆∞u: ∆∞u ti√™n d√πng s·ªë c√≤n thi·∫øu tr∆∞·ªõc
        counter_to_use = None
        
        # ∆Øu ti√™n 1: D√πng s·ªë t·ª´ danh s√°ch c√°c s·ªë c√≤n thi·∫øu
        if missing_image_numbers:
            counter_to_use = missing_image_numbers.pop(0)
        else:
            # ∆Øu ti√™n 2: D√πng s·ªë ti·∫øp theo t·ª´ counter
            temp_counter = save_image_counter
            max_attempts = 1000
            attempts = 0
            
            while attempts < max_attempts:
                filename_check = f"{temp_counter}.jpg"
                filepath_check = os.path.join(SAVE_DIR, filename_check)
                
                if not os.path.exists(filepath_check):
                    counter_to_use = temp_counter
                    save_image_counter = temp_counter + 1
                    break
                
                temp_counter += 1
                attempts += 1
            
            if attempts >= max_attempts:
                print(f"‚úó Kh√¥ng th·ªÉ t√¨m ƒë∆∞·ª£c t√™n file tr·ªëng sau {max_attempts} l·∫ßn th·ª≠")
                return
        
        # T·∫°o t√™n file v√† ƒë∆∞·ªùng d·∫´n
        filename = f"{counter_to_use}.jpg"
        filepath = os.path.join(SAVE_DIR, filename)
        
        # Ki·ªÉm tra l·∫°i m·ªôt l·∫ßn n·ªØa ƒë·ªÉ ƒë·∫£m b·∫£o an to√†n
        if os.path.exists(filepath):
            print(f"‚ö† File {filename} ƒë√£ t·ªìn t·∫°i, b·ªè qua l·∫ßn l∆∞u n√†y")
            return
        
        # L∆∞u ·∫£nh
        cv2.imwrite(filepath, random_crop)
        
        # C·∫≠p nh·∫≠t th·ªùi gian
        last_save_time = current_time
        
        # Hi·ªÉn th·ªã th√¥ng b√°o
        if notification_label:
            notification_label.config(text=f"‚úì ƒê√£ ch·ª•p: {filename}", fg='#00ff00')
        
        # H·ªßy timer c≈© n·∫øu c√≥
        if notification_timer:
            root.after_cancel(notification_timer)
        
        # T·ª± ƒë·ªông ·∫©n th√¥ng b√°o sau 2 gi√¢y
        def restore_auto_save_status():
            if notification_label:
                if SAVE_HAND_IMAGES:
                    notification_label.config(text="‚úì Auto-save: ON", fg='#00ff00')
                else:
                    notification_label.config(text="‚úó Auto-save: OFF", fg='#ff6b6b')
        notification_timer = root.after(2000, restore_auto_save_status)
        
        if is_resized:
            print(f"‚úì ƒê√£ ch·ª•p v√† resize ·∫£nh: {filepath} (t·ª´ {frame_w}x{frame_h} l√™n {target_size}x{target_size})")
        else:
            print(f"‚úì ƒê√£ ch·ª•p ·∫£nh ng·∫´u nhi√™n: {filepath} (v·ªã tr√≠: x={random_x}, y={random_y})")
        
    except Exception as e:
        print(f"‚úó L·ªói khi ch·ª•p ·∫£nh ng·∫´u nhi√™n: {e}")

# ---------- Continuous Random Capture ----------
def start_continuous_capture():
    """B·∫≠t ch·∫ø ƒë·ªô ch·ª•p ng·∫´u nhi√™n li√™n t·ª•c."""
    global auto_capture_enabled, auto_capture_job, notification_label, notification_timer, root
    if auto_capture_enabled:
        print("‚ö† ƒê√£ b·∫≠t ch·ª•p li√™n t·ª•c r·ªìi")
        return
    auto_capture_enabled = True
    print(f"‚úì B·∫≠t ch·ª•p ng·∫´u nhi√™n li√™n t·ª•c (m·ªói {SAVE_INTERVAL}s)")
    
    # Hi·ªÉn th·ªã th√¥ng b√°o trong UI
    if notification_label:
        notification_label.config(text=f"‚úì Ch·ª•p li√™n t·ª•c: ON ({SAVE_INTERVAL}s)", fg='#00ff00')
        if notification_timer:
            root.after_cancel(notification_timer)
        def restore_status():
            if notification_label:
                if SAVE_HAND_IMAGES:
                    notification_label.config(text="‚úì Auto-save: ON", fg='#00ff00')
                else:
                    notification_label.config(text="‚úó Auto-save: OFF", fg='#ff6b6b')
        notification_timer = root.after(3000, restore_status)
    
    _schedule_next_capture()


def stop_continuous_capture():
    """T·∫Øt ch·∫ø ƒë·ªô ch·ª•p ng·∫´u nhi√™n li√™n t·ª•c."""
    global auto_capture_enabled, auto_capture_job, notification_label, notification_timer, root
    auto_capture_enabled = False
    if auto_capture_job and root:
        try:
            root.after_cancel(auto_capture_job)
        except Exception:
            pass
    auto_capture_job = None
    print("‚úì T·∫Øt ch·ª•p ng·∫´u nhi√™n li√™n t·ª•c")
    
    # Hi·ªÉn th·ªã th√¥ng b√°o trong UI
    if notification_label:
        notification_label.config(text="‚úó Ch·ª•p li√™n t·ª•c: OFF", fg='#ff6b6b')
        if notification_timer:
            root.after_cancel(notification_timer)
        def restore_status():
            if notification_label:
                if SAVE_HAND_IMAGES:
                    notification_label.config(text="‚úì Auto-save: ON", fg='#00ff00')
                else:
                    notification_label.config(text="‚úó Auto-save: OFF", fg='#ff6b6b')
        notification_timer = root.after(2000, restore_status)


def _schedule_next_capture():
    """L√™n l·ªãch ch·ª•p ti·∫øp theo n·∫øu ch·∫ø ƒë·ªô li√™n t·ª•c ƒëang b·∫≠t."""
    global auto_capture_job
    if not auto_capture_enabled or stop_flag.is_set() or root is None:
        return
    try:
        # Ch·ª•p ·∫£nh v·ªõi silent=True ƒë·ªÉ tr√°nh spam console
        capture_random_image(silent=True)
    except Exception as e:
        print(f"‚úó L·ªói khi ch·ª•p li√™n t·ª•c: {e}")
    # L·∫∑p l·∫°i sau SAVE_INTERVAL (ms)
    delay_ms = max(int(SAVE_INTERVAL * 1000), 10)
    auto_capture_job = root.after(delay_ms, _schedule_next_capture)


def toggle_continuous_capture():
    """Toggle ch·ª•p ng·∫´u nhi√™n li√™n t·ª•c."""
    if auto_capture_enabled:
        stop_continuous_capture()
    else:
        start_continuous_capture()

# ---------- Tkinter UI Setup ----------
try:
    root = tk.Tk()
    root.title("MediaPipe Hand Landmarker - Real-time Detection")
    
    # T√≠nh to√°n k√≠ch th∆∞·ªõc window
    INFO_PANEL_WIDTH = 350
    total_width = WINDOW_WIDTH + INFO_PANEL_WIDTH + 40
    total_height = WINDOW_HEIGHT + 100
    root.geometry(f"{total_width}x{total_height}")
    root.configure(bg='#1e1e1e')  # Dark background
    root.minsize(800, 500)  # K√≠ch th∆∞·ªõc t·ªëi thi·ªÉu
    
    # CƒÉn gi·ªØa window tr√™n m√†n h√¨nh khi kh·ªüi ƒë·ªông
    root.update_idletasks()
    screen_width = root.winfo_screenwidth()
    screen_height = root.winfo_screenheight()
    x = (screen_width - root.winfo_width()) // 2
    y = (screen_height - root.winfo_height()) // 2 - 35
    root.geometry(f"+{x}+{y}")
    
    # ========== HEADER ==========
    header_frame = tk.Frame(root, bg='#2d2d2d', height=50)
    header_frame.pack(fill=tk.X, padx=0, pady=0)
    header_frame.pack_propagate(False)
    
    title_label = tk.Label(
        header_frame,
        text="MediaPipe Hand Landmarker",
        font=('Segoe UI', 16, 'bold'),
        bg='#2d2d2d',
        fg='#ffffff'
    )
    title_label.pack(side=tk.LEFT, padx=15, pady=10)
    
    status_label = tk.Label(
        header_frame,
        text="‚óè Ready",
        font=('Segoe UI', 10),
        bg='#2d2d2d',
        fg='#00ff00'
    )
    status_label.pack(side=tk.RIGHT, padx=15, pady=10)
    
    # Notification label ƒë·ªÉ hi·ªÉn th·ªã th√¥ng b√°o l∆∞u ·∫£nh
    notification_label = tk.Label(
        header_frame,
        text="",
        font=('Segoe UI', 11, 'bold'),
        bg='#2d2d2d',
        fg='#00ff00'
    )
    notification_label.pack(side=tk.RIGHT, padx=10, pady=10)
    
    # Hi·ªÉn th·ªã tr·∫°ng th√°i auto-save ban ƒë·∫ßu khi kh·ªüi ƒë·ªông
    if SAVE_HAND_IMAGES:
        notification_label.config(text="‚úì Auto-save: ON", fg='#00ff00')
    else:
        notification_label.config(text="‚úó Auto-save: OFF", fg='#ff6b6b')
    
    # ========== MAIN CONTENT AREA ==========
    main_frame = tk.Frame(root, bg='#1e1e1e')
    main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
    
    # ========== LEFT SIDE: INFO PANEL ==========
    info_panel = tk.Frame(main_frame, bg='#252525', width=INFO_PANEL_WIDTH)
    info_panel.pack(side=tk.LEFT, fill=tk.Y, padx=(0, 10))
    info_panel.pack_propagate(False)
    
    # Title cho info panel
    info_title = tk.Label(
        info_panel,
        text="Performance Metrics",
        font=('Segoe UI', 12, 'bold'),
        bg='#252525',
        fg='#ffffff',
        anchor='w'
    )
    info_title.pack(fill=tk.X, padx=15, pady=(15, 10))
    
    # Metrics container v·ªõi vertical layout
    metrics_container = tk.Frame(info_panel, bg='#252525')
    metrics_container.pack(fill=tk.BOTH, expand=True, padx=15, pady=(0, 15))
    
    # Metrics labels (s·∫Ω ƒë∆∞·ª£c update trong update_frame)
    metrics_labels = {}
    metric_configs = [
        ('target_fps', 'Target FPS', '#00a8ff'),
        ('latency', 'Latency', '#00ff00'),
        ('inference_time', 'Inference Time', '#ffff00'),
        ('objects', 'Objects Detected', '#ff6b6b'),
        ('input_fps', 'Input FPS', '#ffa500'),
        ('inference_fps', 'MediaPipe FPS', '#00a8ff'),
        ('display_fps', 'Display FPS', '#00ff00'),
    ]
    
    # T·∫°o vertical layout cho metrics
    for key, label, color in metric_configs:
        # Metric container
        metric_frame = tk.Frame(metrics_container, bg='#252525')
        metric_frame.pack(fill=tk.X, pady=8)
        
        # Label name
        name_label = tk.Label(
            metric_frame,
            text=f"{label}:",
            font=('Segoe UI', 9),
            bg='#252525',
            fg='#aaaaaa',
            anchor='w'
        )
        name_label.pack(anchor='w', padx=(0, 5))
        
        # Value label
        value_label = tk.Label(
            metric_frame,
            text="--",
            font=('Consolas', 11, 'bold'),
            bg='#252525',
            fg=color,
            anchor='w'
        )
        value_label.pack(anchor='w')
        
        metrics_labels[key] = value_label
    
    # ========== RIGHT SIDE: VIDEO DISPLAY ==========
    video_panel = tk.Frame(main_frame, bg='#1e1e1e')
    video_panel.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True)
    
    # Video container v·ªõi border
    video_container = tk.Frame(video_panel, bg='#000000', relief=tk.RAISED, bd=2)
    video_container.pack(fill=tk.BOTH, expand=True)
    
    # Video label (s·∫Ω fill to√†n b·ªô container)
    video_label = tk.Label(
        video_container,
        bg='#000000',
        text="Initializing camera...",
        fg='#888888',
        font=('Segoe UI', 12),
        anchor=tk.CENTER,
        justify=tk.CENTER
    )
    video_label.pack(fill=tk.BOTH, expand=True)
    
    # Callback ƒë·ªÉ update cached container size khi window resize
    def update_container_cache(event=None):
        global cached_container_size
        try:
            w = video_container.winfo_width()
            h = video_container.winfo_height()
            if w > 1 and h > 1:
                cached_container_size['w'] = w
                cached_container_size['h'] = h
        except Exception:
            pass
    
    # Bind resize event ƒë·ªÉ update cache
    video_container.bind('<Configure>', update_container_cache)
    root.bind('<Configure>', update_container_cache)
    
    # ========== KEYBOARD SHORTCUTS ==========
    def toggle_pause():
        """Toggle pause/resume detection"""
        global is_paused
        is_paused = not is_paused
        if status_label:
            if is_paused:
                status_label.config(text="‚óè Paused", fg='#ffa500')
            else:
                status_label.config(text="‚óè Running", fg='#00ff00')
    
    def toggle_save_images():
        """Toggle auto-save hand images"""
        global SAVE_HAND_IMAGES, notification_label, notification_timer
        global missing_image_numbers, save_image_counter
        
        SAVE_HAND_IMAGES = not SAVE_HAND_IMAGES
        
        # Khi b·∫≠t auto-save, qu√©t l·∫°i th∆∞ m·ª•c ƒë·ªÉ t√¨m c√°c s·ªë c√≤n thi·∫øu
        if SAVE_HAND_IMAGES:
            missing_image_numbers, save_image_counter = scan_missing_image_numbers(SAVE_DIR)
            if missing_image_numbers:
                print(f"‚úì ƒê√£ t√¨m th·∫•y {len(missing_image_numbers)} s·ªë c√≤n thi·∫øu: {missing_image_numbers}")
                print(f"  S·∫Ω ∆∞u ti√™n l·∫•p v√†o c√°c s·ªë n√†y tr∆∞·ªõc khi ti·∫øp t·ª•c t·ª´ s·ªë {save_image_counter}")
            elif save_image_counter > 0:
                print(f"‚úì ƒê√£ t√¨m th·∫•y {save_image_counter} ·∫£nh trong th∆∞ m·ª•c. B·∫Øt ƒë·∫ßu t·ª´ s·ªë {save_image_counter}")
            else:
                print(f"‚úì Th∆∞ m·ª•c tr·ªëng. B·∫Øt ƒë·∫ßu t·ª´ s·ªë 0")
        
        if notification_label:
            if SAVE_HAND_IMAGES:
                notification_label.config(text="‚úì Auto-save: ON", fg='#00ff00')
            else:
                notification_label.config(text="‚úó Auto-save: OFF", fg='#ff6b6b')
        
        # H·ªßy timer c≈© n·∫øu c√≥
        if notification_timer:
            root.after_cancel(notification_timer)
        
        # Kh√¥ng t·ª± ƒë·ªông ·∫©n tr·∫°ng th√°i auto-save (lu√¥n hi·ªÉn th·ªã ƒë·ªÉ ng∆∞·ªùi d√πng bi·∫øt tr·∫°ng th√°i)
        # Tr·∫°ng th√°i s·∫Ω ch·ªâ b·ªã thay th·∫ø t·∫°m th·ªùi khi c√≥ th√¥ng b√°o l∆∞u ·∫£nh
        
        print(f"‚úì Auto-save images: {'ON' if SAVE_HAND_IMAGES else 'OFF'}")
    
    # Bind keyboard shortcuts
    root.bind('<space>', lambda e: toggle_pause())
    root.bind('<KeyPress-x>', lambda e: toggle_save_images())
    root.bind('<KeyPress-X>', lambda e: toggle_save_images())
    
    # Bind ph√≠m C ƒë·ªÉ b·∫≠t/t·∫Øt ch·ª•p ·∫£nh ng·∫´u nhi√™n li√™n t·ª•c
    def handle_capture(e=None):
        print("üîç Ph√≠m C ƒë∆∞·ª£c nh·∫•n - B·∫≠t/t·∫Øt ch·ª•p li√™n t·ª•c")
        try:
            toggle_continuous_capture()
        except NameError:
            print("‚úó L·ªói: H√†m toggle_continuous_capture() ch∆∞a ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a")
        except Exception as ex:
            print(f"‚úó L·ªói khi toggle ch·ª•p li√™n t·ª•c: {ex}")
    
    # Bind nhi·ªÅu c√°ch ƒë·ªÉ ƒë·∫£m b·∫£o ho·∫°t ƒë·ªông tr√™n m·ªçi h·ªá th·ªëng
    root.bind('<Key-c>', handle_capture)
    root.bind('<Key-C>', handle_capture)
    root.bind('<c>', handle_capture)
    root.bind('<C>', handle_capture)
    root.bind_all('<Key-c>', handle_capture)  # Bind to√†n c·ª•c
    root.bind_all('<Key-C>', handle_capture)   # Bind to√†n c·ª•c
    
    # ƒê·∫£m b·∫£o root lu√¥n nh·∫≠n focus ƒë·ªÉ nh·∫≠n keyboard events
    root.focus_set()
    root.focus_force()  # Force focus
    
    # Bind event ƒë·ªÉ ƒë·∫£m b·∫£o focus khi click v√†o window
    def on_focus_in(e):
        root.focus_set()
    root.bind('<FocusIn>', on_focus_in)
    root.bind('<Button-1>', lambda e: root.focus_set())
    
    # Handle window close
    def on_closing():
        print("\nStopped by user (closed window)")
        stop_flag.set()
        root.quit()
        root.destroy()
    
    root.protocol("WM_DELETE_WINDOW", on_closing)
    
    # ========== SETTINGS PANEL ==========
    settings_window = None
    
    def open_settings():
        """Open settings window"""
        global settings_window
        
        if settings_window is not None:
            try:
                if settings_window.winfo_exists():
                    settings_window.lift()
                    settings_window.focus()
                    return
            except Exception:
                pass
        
        settings_window = tk.Toplevel(root)
        settings_window.title("Settings")
        settings_window.geometry("600x550")
        settings_window.configure(bg='#1e1e1e')
        settings_window.resizable(False, False)
        
        # Center settings window
        settings_window.update_idletasks()
        x = (settings_window.winfo_screenwidth() // 2) - (600 // 2)
        y = (settings_window.winfo_screenheight() // 2) - (550 // 2)
        settings_window.geometry(f"+{x}+{y}")
        
        # Header
        header = tk.Label(
            settings_window,
            text="Settings",
            font=('Segoe UI', 16, 'bold'),
            bg='#2d2d2d',
            fg='#ffffff',
            pady=15
        )
        header.pack(fill=tk.X)
        
        # Content frame
        content_frame = tk.Frame(settings_window, bg='#1e1e1e', padx=20, pady=20)
        content_frame.pack(fill=tk.BOTH, expand=True)
        
        # Container cho 2 c·ªôt
        columns_frame = tk.Frame(content_frame, bg='#1e1e1e')
        columns_frame.pack(fill=tk.BOTH, expand=True)
        
        # ========== COLUMN 1: Performance Settings ==========
        perf_frame = tk.LabelFrame(
            columns_frame,
            text="Performance Settings",
            font=('Segoe UI', 11, 'bold'),
            bg='#252525',
            fg='#ffffff',
            padx=15,
            pady=15
        )
        perf_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(0, 10))
        
        # NUM_HANDS
        tk.Label(perf_frame, text="Number of Hands:", bg='#252525', fg='#aaaaaa', anchor='w').pack(fill=tk.X, pady=5)
        num_hands_var = tk.IntVar(value=NUM_HANDS)
        num_hands_scale = tk.Scale(
            perf_frame,
            from_=1,
            to=10,
            orient=tk.HORIZONTAL,
            variable=num_hands_var,
            bg='#252525',
            fg='#ffffff',
            highlightbackground='#252525'
        )
        num_hands_scale.pack(fill=tk.X, pady=5)
        
        # MIN_DETECTION_CONFIDENCE
        tk.Label(perf_frame, text="Min Detection Confidence:", bg='#252525', fg='#aaaaaa', anchor='w').pack(fill=tk.X, pady=5)
        min_det_var = tk.DoubleVar(value=MIN_DETECTION_CONFIDENCE)
        min_det_scale = tk.Scale(
            perf_frame,
            from_=0.1,
            to=1.0,
            resolution=0.1,
            orient=tk.HORIZONTAL,
            variable=min_det_var,
            bg='#252525',
            fg='#ffffff',
            highlightbackground='#252525'
        )
        min_det_scale.pack(fill=tk.X, pady=5)
        
        # MIN_TRACKING_CONFIDENCE
        tk.Label(perf_frame, text="Min Tracking Confidence:", bg='#252525', fg='#aaaaaa', anchor='w').pack(fill=tk.X, pady=5)
        min_track_var = tk.DoubleVar(value=MIN_TRACKING_CONFIDENCE)
        min_track_scale = tk.Scale(
            perf_frame,
            from_=0.1,
            to=1.0,
            resolution=0.1,
            orient=tk.HORIZONTAL,
            variable=min_track_var,
            bg='#252525',
            fg='#ffffff',
            highlightbackground='#252525'
        )
        min_track_scale.pack(fill=tk.X, pady=5)
        
        # MIN_PRESENCE_CONFIDENCE
        tk.Label(perf_frame, text="Min Presence Confidence:", bg='#252525', fg='#aaaaaa', anchor='w').pack(fill=tk.X, pady=5)
        min_presence_var = tk.DoubleVar(value=MIN_PRESENCE_CONFIDENCE)
        min_presence_scale = tk.Scale(
            perf_frame,
            from_=0.1,
            to=1.0,
            resolution=0.1,
            orient=tk.HORIZONTAL,
            variable=min_presence_var,
            bg='#252525',
            fg='#ffffff',
            highlightbackground='#252525'
        )
        min_presence_scale.pack(fill=tk.X, pady=5)
        
        # ========== COLUMN 2: EMA Settings ==========
        ema_frame = tk.LabelFrame(
            columns_frame,
            text="EMA Smoothing",
            font=('Segoe UI', 11, 'bold'),
            bg='#252525',
            fg='#ffffff',
            padx=15,
            pady=15
        )
        ema_frame.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True, padx=(10, 0))
        
        # ENABLE_EMA_SMOOTHING
        ema_enable_var = tk.BooleanVar(value=ENABLE_EMA_SMOOTHING)
        tk.Checkbutton(
            ema_frame,
            text="Enable EMA Smoothing",
            variable=ema_enable_var,
            bg='#252525',
            fg='#ffffff',
            selectcolor='#1e1e1e',
            activebackground='#252525',
            activeforeground='#ffffff'
        ).pack(anchor='w', pady=5)
        
        # EMA_ALPHA
        tk.Label(ema_frame, text="EMA Alpha:", bg='#252525', fg='#aaaaaa', anchor='w').pack(fill=tk.X, pady=5)
        ema_alpha_var = tk.DoubleVar(value=EMA_ALPHA)
        ema_alpha_scale = tk.Scale(
            ema_frame,
            from_=0.1,
            to=1.0,
            resolution=0.1,
            orient=tk.HORIZONTAL,
            variable=ema_alpha_var,
            bg='#252525',
            fg='#ffffff',
            highlightbackground='#252525'
        )
        ema_alpha_scale.pack(fill=tk.X, pady=5)
        
        # Separator
        separator1 = tk.Frame(ema_frame, bg='#3d3d3d', height=1)
        separator1.pack(fill=tk.X, pady=15)
        
        # Auto-save Settings
        save_title = tk.Label(
            ema_frame,
            text="Auto-save Settings",
            font=('Segoe UI', 10, 'bold'),
            bg='#252525',
            fg='#ffffff',
            anchor='w'
        )
        save_title.pack(anchor='w', pady=(0, 5))
        
        # SAVE_HAND_IMAGES
        save_images_var = tk.BooleanVar(value=SAVE_HAND_IMAGES)
        tk.Checkbutton(
            ema_frame,
            text="Auto-save Hand Images",
            variable=save_images_var,
            bg='#252525',
            fg='#ffffff',
            selectcolor='#1e1e1e',
            activebackground='#252525',
            activeforeground='#ffffff'
        ).pack(anchor='w', pady=5)
        
        # Save Directory Selection
        save_dir_frame = tk.Frame(ema_frame, bg='#252525')
        save_dir_frame.pack(fill=tk.X, pady=(10, 5))
        
        tk.Label(
            save_dir_frame,
            text="Th∆∞ m·ª•c l∆∞u ·∫£nh:",
            bg='#252525',
            fg='#aaaaaa',
            anchor='w',
            font=('Segoe UI', 9)
        ).pack(anchor='w', pady=(0, 5))
        
        # Frame ch·ª©a ƒë∆∞·ªùng d·∫´n v√† n√∫t
        save_dir_path_frame = tk.Frame(save_dir_frame, bg='#252525')
        save_dir_path_frame.pack(fill=tk.X)
        
        # Label hi·ªÉn th·ªã ƒë∆∞·ªùng d·∫´n (c√≥ th·ªÉ cu·ªôn n·∫øu d√†i)
        save_dir_label = tk.Label(
            save_dir_path_frame,
            text=SAVE_DIR,
            bg='#1e1e1e',
            fg='#ffffff',
            anchor='w',
            font=('Consolas', 8),
            relief=tk.SUNKEN,
            bd=1,
            padx=5,
            pady=3,
            wraplength=250
        )
        save_dir_label.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=(0, 5))
        
        # N√∫t ch·ªçn th∆∞ m·ª•c
        def browse_save_dir():
            """M·ªü dialog ch·ªçn th∆∞ m·ª•c l∆∞u ·∫£nh"""
            global SAVE_DIR, missing_image_numbers, save_image_counter, SAVE_HAND_IMAGES
            selected_dir = filedialog.askdirectory(
                title="Ch·ªçn th∆∞ m·ª•c l∆∞u ·∫£nh",
                initialdir=SAVE_DIR if os.path.exists(SAVE_DIR) else script_dir
            )
            if selected_dir:  # N·∫øu ng∆∞·ªùi d√πng ch·ªçn th∆∞ m·ª•c (kh√¥ng cancel)
                SAVE_DIR = selected_dir
                # C·∫≠p nh·∫≠t label hi·ªÉn th·ªã
                save_dir_label.config(text=SAVE_DIR)
                print(f"‚úì ƒê√£ ch·ªçn th∆∞ m·ª•c l∆∞u ·∫£nh: {SAVE_DIR}")
                
                # N·∫øu auto-save ƒëang b·∫≠t, qu√©t l·∫°i th∆∞ m·ª•c m·ªõi ƒë·ªÉ t√¨m c√°c s·ªë c√≤n thi·∫øu
                if SAVE_HAND_IMAGES:
                    missing_image_numbers, save_image_counter = scan_missing_image_numbers(SAVE_DIR)
                    if missing_image_numbers:
                        print(f"‚úì ƒê√£ t√¨m th·∫•y {len(missing_image_numbers)} s·ªë c√≤n thi·∫øu: {missing_image_numbers}")
                        print(f"  S·∫Ω ∆∞u ti√™n l·∫•p v√†o c√°c s·ªë n√†y tr∆∞·ªõc khi ti·∫øp t·ª•c t·ª´ s·ªë {save_image_counter}")
                    elif save_image_counter > 0:
                        print(f"‚úì ƒê√£ t√¨m th·∫•y {save_image_counter} ·∫£nh trong th∆∞ m·ª•c. B·∫Øt ƒë·∫ßu t·ª´ s·ªë {save_image_counter}")
                    else:
                        print(f"‚úì Th∆∞ m·ª•c tr·ªëng. B·∫Øt ƒë·∫ßu t·ª´ s·ªë 0")
        
        browse_btn = tk.Button(
            save_dir_path_frame,
            text="üìÅ Ch·ªçn",
            command=browse_save_dir,
            bg='#00a8ff',
            fg='#ffffff',
            font=('Segoe UI', 9),
            padx=10,
            pady=3,
            cursor='hand2',
            relief=tk.RAISED,
            bd=1
        )
        browse_btn.pack(side=tk.RIGHT)
        
        # Buttons (lu√¥n ·ªü bottom, kh√¥ng b·ªã che)
        button_frame = tk.Frame(settings_window, bg='#1e1e1e', pady=15, padx=20)
        button_frame.pack(fill=tk.X, side=tk.BOTTOM)
        
        def apply_settings():
            """Apply settings changes"""
            global NUM_HANDS, MIN_DETECTION_CONFIDENCE, MIN_PRESENCE_CONFIDENCE, MIN_TRACKING_CONFIDENCE
            global ENABLE_EMA_SMOOTHING, EMA_ALPHA, landmarker, SAVE_HAND_IMAGES, SAVE_DIR
            global missing_image_numbers, save_image_counter
            
            new_num_hands = num_hands_var.get()
            new_min_det = min_det_var.get()
            new_min_track = min_track_var.get()
            new_min_presence = min_presence_var.get()
            new_ema_enable = ema_enable_var.get()
            new_ema_alpha = ema_alpha_var.get()
            new_save_images = save_images_var.get()
            
            # √Åp d·ª•ng EMA settings ngay
            ENABLE_EMA_SMOOTHING = new_ema_enable
            EMA_ALPHA = new_ema_alpha
            
            # √Åp d·ª•ng Auto-save settings ngay
            old_save_state = SAVE_HAND_IMAGES
            old_save_dir = SAVE_DIR
            SAVE_HAND_IMAGES = new_save_images
            
            # N·∫øu th∆∞ m·ª•c l∆∞u ·∫£nh thay ƒë·ªïi ho·∫∑c b·∫≠t auto-save (t·ª´ OFF sang ON), qu√©t l·∫°i th∆∞ m·ª•c
            if (SAVE_HAND_IMAGES and not old_save_state) or (SAVE_HAND_IMAGES and SAVE_DIR != old_save_dir):
                missing_image_numbers, save_image_counter = scan_missing_image_numbers(SAVE_DIR)
                if missing_image_numbers:
                    print(f"‚úì ƒê√£ t√¨m th·∫•y {len(missing_image_numbers)} s·ªë c√≤n thi·∫øu: {missing_image_numbers}")
                    print(f"  S·∫Ω ∆∞u ti√™n l·∫•p v√†o c√°c s·ªë n√†y tr∆∞·ªõc khi ti·∫øp t·ª•c t·ª´ s·ªë {save_image_counter}")
                elif save_image_counter > 0:
                    print(f"‚úì ƒê√£ t√¨m th·∫•y {save_image_counter} ·∫£nh trong th∆∞ m·ª•c. B·∫Øt ƒë·∫ßu t·ª´ s·ªë {save_image_counter}")
                else:
                    print(f"‚úì Th∆∞ m·ª•c tr·ªëng. B·∫Øt ƒë·∫ßu t·ª´ s·ªë 0")
            
            # Ki·ªÉm tra xem c√≥ c·∫ßn recreate landmarker kh√¥ng
            need_recreate = (
                NUM_HANDS != new_num_hands or
                MIN_DETECTION_CONFIDENCE != new_min_det or
                MIN_PRESENCE_CONFIDENCE != new_min_presence or
                MIN_TRACKING_CONFIDENCE != new_min_track
            )
            
            if need_recreate:
                # Update global variables
                NUM_HANDS = new_num_hands
                MIN_DETECTION_CONFIDENCE = new_min_det
                MIN_PRESENCE_CONFIDENCE = new_min_presence
                MIN_TRACKING_CONFIDENCE = new_min_track
                
                # Recreate landmarker v·ªõi options m·ªõi
                try:
                    # ƒê√≥ng landmarker c≈©
                    if landmarker:
                        landmarker.close()
                    
                    # T·∫°o options m·ªõi
                    new_options = HandLandmarkerOptions(
                        base_options=base_options,
                        running_mode=VisionRunningMode.VIDEO,
                        num_hands=NUM_HANDS,
                        min_hand_detection_confidence=MIN_DETECTION_CONFIDENCE,
                        min_hand_presence_confidence=MIN_PRESENCE_CONFIDENCE,
                        min_tracking_confidence=MIN_TRACKING_CONFIDENCE,
                    )
                    
                    # T·∫°o landmarker m·ªõi
                    landmarker = HandLandmarker.create_from_options(new_options)
                    
                    # Warm-up landmarker m·ªõi
                    dummy_frame = np.zeros((480, 640, 3), dtype=np.uint8)
                    dummy_frame.flags.writeable = False
                    dummy_mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=dummy_frame)
                    landmarker.detect_for_video(dummy_mp_image, 0)
                    
                    print(f"‚úì Landmarker recreated with new settings:")
                    print(f"  NUM_HANDS={NUM_HANDS}, MIN_DET={MIN_DETECTION_CONFIDENCE:.2f}, "
                          f"MIN_PRESENCE={MIN_PRESENCE_CONFIDENCE:.2f}, MIN_TRACK={MIN_TRACKING_CONFIDENCE:.2f}")
                except Exception as e:
                    print(f"‚úó Error recreating landmarker: {e}")
                    return
            
            print(f"‚úì Settings applied:")
            print(f"  NUM_HANDS={NUM_HANDS}, MIN_DET={MIN_DETECTION_CONFIDENCE:.2f}, "
                  f"MIN_PRESENCE={MIN_PRESENCE_CONFIDENCE:.2f}, MIN_TRACK={MIN_TRACKING_CONFIDENCE:.2f}")
            print(f"  EMA={ENABLE_EMA_SMOOTHING}, ALPHA={EMA_ALPHA:.2f}")
            print(f"  Auto-save Images={SAVE_HAND_IMAGES}")
            print(f"  Save Directory={SAVE_DIR}")
        
        def close_settings():
            """Close settings window"""
            global settings_window
            if settings_window:
                settings_window.destroy()
            settings_window = None
        
        tk.Button(
            button_frame,
            text="Apply",
            command=apply_settings,
            bg='#00a8ff',
            fg='#ffffff',
            font=('Segoe UI', 10, 'bold'),
            padx=20,
            pady=5,
            cursor='hand2'
        ).pack(side=tk.LEFT, padx=5)
        
        tk.Button(
            button_frame,
            text="Close",
            command=close_settings,
            bg='#666666',
            fg='#ffffff',
            font=('Segoe UI', 10),
            padx=20,
            pady=5,
            cursor='hand2'
        ).pack(side=tk.LEFT, padx=5)
        
        # Handle close
        settings_window.protocol("WM_DELETE_WINDOW", close_settings)
    
    # Settings button in header
    settings_btn = tk.Button(
        header_frame,
        text="‚öô Settings",
        command=open_settings,
        bg='#2d2d2d',
        fg='#ffffff',
        font=('Segoe UI', 9),
        relief=tk.FLAT,
        padx=10,
        pady=5,
        cursor='hand2',
        activebackground='#3d3d3d',
        activeforeground='#ffffff'
    )
    settings_btn.pack(side=tk.RIGHT, padx=5)
    
    # Capture button in header (ƒë·ªÉ b·∫≠t/t·∫Øt ch·ª•p li√™n t·ª•c)
    capture_btn = tk.Button(
        header_frame,
        text="üì∑ Ch·ª•p li√™n t·ª•c (C)",
        command=toggle_continuous_capture,
        bg='#2d2d2d',
        fg='#00ff00',
        font=('Segoe UI', 9),
        relief=tk.FLAT,
        padx=10,
        pady=5,
        cursor='hand2',
        activebackground='#3d3d3d',
        activeforeground='#00ff00'
    )
    capture_btn.pack(side=tk.RIGHT, padx=5)
    
    print("‚úì Tkinter UI initialized")
except Exception as e:
    raise RuntimeError(f"Kh√¥ng th·ªÉ kh·ªüi t·∫°o Tkinter UI: {e}") from e

current_photo = None

# ---------- Helper Functions ----------
def limit_list_size(data_list, max_size):
    """Gi·ªõi h·∫°n k√≠ch th∆∞·ªõc list, ch·ªâ gi·ªØ N gi√° tr·ªã g·∫ßn nh·∫•t"""
    if len(data_list) > max_size:
        return data_list[-max_size:]
    return data_list

def fps_text(val, avg=None):
    """Format FPS text v·ªõi optional average value"""
    return f"{val:.1f} (avg: {avg:.1f})" if avg is not None else f"{val:.1f}"

def ms_text(val, avg=None):
    """Format milliseconds text v·ªõi optional average value"""
    return f"{val:.1f}ms (avg: {avg:.1f}ms)" if avg is not None else f"{val:.1f}ms"

def moving_avg(data_list, window=30):
    """T√≠nh trung b√¨nh tr∆∞·ª£t (moving average)"""
    if not data_list:
        return None
    return sum(data_list[-window:]) / min(window, len(data_list))

def get_track_color(track_id):
    """T·∫°o m√†u ·ªïn ƒë·ªãnh t·ª´ track_id"""
    hash_val = hash(str(track_id)) % (256**3)
    r = max(100, (hash_val & 0xFF0000) >> 16)
    g = max(100, (hash_val & 0x00FF00) >> 8)
    b = max(100, hash_val & 0x0000FF)
    return (r, g, b)

def save_hand_image(frame, min_x, min_y, max_x, max_y):
    """
    C·∫Øt v√† l∆∞u ·∫£nh b√†n tay v·ªõi k√≠ch th∆∞·ªõc 640x640
    
    Args:
        frame: Frame g·ªëc (BGR)
        min_x, min_y, max_x, max_y: T·ªça ƒë·ªô bounding box c·ªßa b√†n tay
    """
    global save_image_counter, last_save_time, notification_label, notification_timer, root
    global missing_image_numbers
    
    # Ki·ªÉm tra kho·∫£ng th·ªùi gian gi·ªØa c√°c l·∫ßn l∆∞u
    current_time = time.time()
    if current_time - last_save_time < SAVE_INTERVAL:
        return
    
    try:
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
        os.makedirs(SAVE_DIR, exist_ok=True)
        
        # C·∫Øt ·∫£nh t·ª´ bounding box (th√™m padding nh·ªè ƒë·ªÉ kh√¥ng b·ªã c·∫Øt)
        padding = 20
        frame_h, frame_w = frame.shape[:2]
        
        # T√≠nh to√°n t·ªça ƒë·ªô v·ªõi padding
        crop_x1 = max(0, min_x - padding)
        crop_y1 = max(0, min_y - padding)
        crop_x2 = min(frame_w, max_x + padding)
        crop_y2 = min(frame_h, max_y + padding)
        
        # C·∫Øt ·∫£nh
        hand_crop = frame[crop_y1:crop_y2, crop_x1:crop_x2]
        
        if hand_crop.size == 0:
            return
        
        # Resize v·ªÅ 640x640 (gi·ªØ t·ª∑ l·ªá v√† pad v·ªõi m√†u ƒëen n·∫øu c·∫ßn)
        target_size = 640
        h, w = hand_crop.shape[:2]
        
        # T√≠nh scale ƒë·ªÉ fit v√†o 640x640
        scale = min(target_size / w, target_size / h)
        new_w = int(w * scale)
        new_h = int(h * scale)
        
        # Resize
        hand_resized = cv2.resize(hand_crop, (new_w, new_h), interpolation=cv2.INTER_AREA)
        
        # T·∫°o ·∫£nh 640x640 v·ªõi background ƒëen
        hand_final = np.zeros((target_size, target_size, 3), dtype=np.uint8)
        
        # ƒê·∫∑t ·∫£nh ƒë√£ resize v√†o gi·ªØa
        y_offset = (target_size - new_h) // 2
        x_offset = (target_size - new_w) // 2
        hand_final[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = hand_resized
        
        # T√¨m s·ªë counter ƒë·ªÉ l∆∞u: ∆∞u ti√™n d√πng s·ªë c√≤n thi·∫øu tr∆∞·ªõc
        counter_to_use = None
        
        # ∆Øu ti√™n 1: D√πng s·ªë t·ª´ danh s√°ch c√°c s·ªë c√≤n thi·∫øu
        if missing_image_numbers:
            counter_to_use = missing_image_numbers.pop(0)  # L·∫•y s·ªë ƒë·∫ßu ti√™n v√† x√≥a kh·ªèi list
        else:
            # ∆Øu ti√™n 2: D√πng s·ªë ti·∫øp theo t·ª´ counter
            # Ki·ªÉm tra xem s·ªë n√†y ƒë√£ t·ªìn t·∫°i ch∆∞a (ph√≤ng tr∆∞·ªùng h·ª£p c√≥ file m·ªõi ƒë∆∞·ª£c th√™m v√†o t·ª´ b√™n ngo√†i)
            temp_counter = save_image_counter
            max_attempts = 1000
            attempts = 0
            
            while attempts < max_attempts:
                filename_check = f"{temp_counter}.jpg"
                filepath_check = os.path.join(SAVE_DIR, filename_check)
                
                # N·∫øu file ch∆∞a t·ªìn t·∫°i, d√πng s·ªë n√†y
                if not os.path.exists(filepath_check):
                    counter_to_use = temp_counter
                    save_image_counter = temp_counter + 1  # C·∫≠p nh·∫≠t counter cho l·∫ßn sau
                    break
                
                # N·∫øu file ƒë√£ t·ªìn t·∫°i, th·ª≠ s·ªë ti·∫øp theo
                temp_counter += 1
                attempts += 1
            
            if attempts >= max_attempts:
                print(f"‚úó Kh√¥ng th·ªÉ t√¨m ƒë∆∞·ª£c t√™n file tr·ªëng sau {max_attempts} l·∫ßn th·ª≠")
                return
        
        # T·∫°o t√™n file v√† ƒë∆∞·ªùng d·∫´n
        filename = f"{counter_to_use}.jpg"
        filepath = os.path.join(SAVE_DIR, filename)
        
        # Ki·ªÉm tra l·∫°i m·ªôt l·∫ßn n·ªØa ƒë·ªÉ ƒë·∫£m b·∫£o an to√†n (ph√≤ng tr∆∞·ªùng h·ª£p c√≥ race condition)
        if os.path.exists(filepath):
            print(f"‚ö† File {filename} ƒë√£ t·ªìn t·∫°i, b·ªè qua l·∫ßn l∆∞u n√†y")
            return
        
        # L∆∞u ·∫£nh
        cv2.imwrite(filepath, hand_final)
        
        # C·∫≠p nh·∫≠t th·ªùi gian
        last_save_time = current_time
        
        # Hi·ªÉn th·ªã th√¥ng b√°o
        if notification_label:
            notification_label.config(text=f"‚úì ƒê√£ l∆∞u: {filename}", fg='#00ff00')
        
        # H·ªßy timer c≈© n·∫øu c√≥
        if notification_timer:
            root.after_cancel(notification_timer)
        
        # T·ª± ƒë·ªông ·∫©n th√¥ng b√°o sau 2 gi√¢y v√† quay l·∫°i hi·ªÉn th·ªã tr·∫°ng th√°i auto-save
        def restore_auto_save_status():
            if notification_label:
                if SAVE_HAND_IMAGES:
                    notification_label.config(text="‚úì Auto-save: ON", fg='#00ff00')
                else:
                    notification_label.config(text="‚úó Auto-save: OFF", fg='#ff6b6b')
        notification_timer = root.after(2000, restore_auto_save_status)
        
        print(f"‚úì ƒê√£ l∆∞u ·∫£nh: {filepath}")
        
    except Exception as e:
        print(f"‚úó L·ªói khi l∆∞u ·∫£nh: {e}")

def draw_keypoints(frame, keypoints, color=(0, 255, 255), radius=3, conf_threshold=0.3):
    """
    V·∫Ω keypoints l√™n frame (t·ªëi ∆∞u cho real-time v·ªõi OpenCV direct calls)
    
    Performance: Custom OpenCV nhanh h∆°n MediaPipe official draw_landmarks v√¨:
    - Kh√¥ng c√≥ protobuf conversion overhead
    - Direct C++ OpenCV backend
    - C√≥ th·ªÉ t·ªëi ∆∞u validation v√† bounds checking
    
    Args:
        frame: Frame ƒë·ªÉ v·∫Ω
        keypoints: numpy array shape (num_keypoints, 3) v·ªõi (x, y, confidence) ho·∫∑c (num_keypoints, 2) v·ªõi (x, y)
        color: M√†u keypoints (BGR)
        radius: B√°n k√≠nh ƒëi·ªÉm keypoint
        conf_threshold: Ng∆∞·ª°ng confidence t·ªëi thi·ªÉu ƒë·ªÉ v·∫Ω keypoint
    """
    if keypoints is None or len(keypoints) == 0:
        return
    
    frame_h, frame_w = frame.shape[:2]
    
    radius_outer = radius + 1
    white = (255, 255, 255)
    
    # Keypoints shape: (num_keypoints, 3) v·ªõi (x, y, confidence) ho·∫∑c (num_keypoints, 2) v·ªõi (x, y)
    for kp in keypoints:
        if len(kp) >= 2:
            x, y = float(kp[0]), float(kp[1])
            conf = float(kp[2]) if len(kp) > 2 else 1.0
            
            # V·∫Ω keypoint n·∫øu confidence ƒë·ªß v√† t·ªça ƒë·ªô h·ª£p l·ªá (>= 0 v√† < frame size)
            if conf >= conf_threshold and 0 <= x < frame_w and 0 <= y < frame_h:
                x, y = int(x), int(y)
                # V·∫Ω ƒëi·ªÉm keypoint v·ªõi vi·ªÅn tr·∫Øng m·ªèng ƒë·ªÉ d·ªÖ nh√¨n
                cv2.circle(frame, (x, y), radius_outer, white, -1)  # Vi·ªÅn tr·∫Øng
                cv2.circle(frame, (x, y), radius, color, -1)  # ƒêi·ªÉm keypoint

def draw_hand_skeleton(frame, keypoints, color=(0, 255, 255), thickness=1, conf_threshold=0.3):
    """
    V·∫Ω skeleton connections cho hand keypoints (21 keypoints cho hand)
    
    C·∫•u tr√∫c 21 keypoints theo MediaPipe:
    - 0: Wrist (c·ªï tay)
    - 1-4: Thumb (ng√≥n c√°i): 1=CMC, 2=MCP, 3=IP, 4=Tip
    - 5-8: Index (ng√≥n tr·ªè): 5=MCP, 6=PIP, 7=DIP, 8=Tip
    - 9-12: Middle (ng√≥n gi·ªØa): 9=MCP, 10=PIP, 11=DIP, 12=Tip
    - 13-16: Ring (ng√≥n √°p √∫t): 13=MCP, 14=PIP, 15=DIP, 16=Tip
    - 17-20: Pinky (ng√≥n √∫t): 17=MCP, 18=PIP, 19=DIP, 20=Tip
    
    Connections n√†y kh·ªõp v·ªõi MediaPipe solutions.hands.HAND_CONNECTIONS
    
    Args:
        frame: Frame ƒë·ªÉ v·∫Ω
        keypoints: numpy array shape (21, 3) v·ªõi (x, y, confidence) ho·∫∑c (21, 2) v·ªõi (x, y)
        color: M√†u ƒë∆∞·ªùng n·ªëi (BGR)
        thickness: ƒê·ªô d√†y ƒë∆∞·ªùng n·ªëi
        conf_threshold: Ng∆∞·ª°ng confidence t·ªëi thi·ªÉu ƒë·ªÉ v·∫Ω connection
    """
    if keypoints is None or len(keypoints) < 21:
        return
    
    frame_h, frame_w = frame.shape[:2]
    
    # Hand keypoint connections theo MediaPipe HAND_CONNECTIONS
    # Wrist to finger bases (CMC cho thumb, MCP cho c√°c ng√≥n kh√°c)
    wrist_to_fingers = [(0, 1), (0, 5), (0, 9), (0, 13), (0, 17)]
    
    # Thumb: CMC -> MCP -> IP -> Tip
    thumb_chain = [(1, 2), (2, 3), (3, 4)]
    
    # Index finger: MCP -> PIP -> DIP -> Tip
    index_chain = [(5, 6), (6, 7), (7, 8)]
    
    # Middle finger: MCP -> PIP -> DIP -> Tip
    middle_chain = [(9, 10), (10, 11), (11, 12)]
    
    # Ring finger: MCP -> PIP -> DIP -> Tip
    ring_chain = [(13, 14), (14, 15), (15, 16)]
    
    # Pinky finger: MCP -> PIP -> DIP -> Tip
    pinky_chain = [(17, 18), (18, 19), (19, 20)]
    
    # T·∫•t c·∫£ connections
    all_connections = wrist_to_fingers + thumb_chain + index_chain + middle_chain + ring_chain + pinky_chain
    
    for start_idx, end_idx in all_connections:
        if start_idx < len(keypoints) and end_idx < len(keypoints):
            kp1 = keypoints[start_idx]
            kp2 = keypoints[end_idx]
            
            if len(kp1) >= 2 and len(kp2) >= 2:
                x1, y1 = float(kp1[0]), float(kp1[1])
                x2, y2 = float(kp2[0]), float(kp2[1])
                conf1 = float(kp1[2]) if len(kp1) > 2 else 1.0
                conf2 = float(kp2[2]) if len(kp2) > 2 else 1.0
                
                if (
                    conf1 >= conf_threshold
                    and conf2 >= conf_threshold
                    and 0 <= x1 < frame_w
                    and 0 <= y1 < frame_h
                    and 0 <= x2 < frame_w
                    and 0 <= y2 < frame_h
                ):
                    pt1 = (int(x1), int(y1))
                    pt2 = (int(x2), int(y2))
                    cv2.line(frame, pt1, pt2, color, thickness)

# ---------- Main Update Loop ----------
def update_frame():
    """Update frame trong Tkinter UI (ch·∫°y trong mainloop)"""
    global frame_count, total_objects, prev_display_time, prev_capture_time
    global latest_detection, current_photo
    global inference_fps_list, inference_times, input_fps_list, fps_list, frame_intervals, display_latencies
    global cached_container_size, cached_metrics_values, is_paused, notification_timer
    global current_frame, current_frame_lock
    
    if stop_flag.is_set():
        if root:
            root.quit()
        return
    
    # Skip update if paused
    if is_paused:
        if root and not stop_flag.is_set():
            root.after(200, update_frame)  # Check l·∫°i sau 200ms
        return
    
    try:
        # L·∫•y frame m·ªõi nh·∫•t t·ª´ display_frame_queue (skip frames c≈© ƒë·ªÉ gi·∫£m lag)
        frame_id, frame_original, frame_time = None, None, None
        
        try:
            # L·∫•y t·∫•t c·∫£ frames v√† ch·ªâ gi·ªØ frame m·ªõi nh·∫•t (skip frames c≈©)
            while True:
                frame_id, frame_original, frame_time = display_frame_queue.get_nowait()
                display_frame_queue.task_done()
        except Empty:
            if frame_original is None:
                try:
                    frame_id, frame_original, frame_time = display_frame_queue.get(timeout=0.01)
                    display_frame_queue.task_done()
                except Empty:
                    # Schedule next update
                    if root and not stop_flag.is_set():
                        root.after(10, update_frame)
                    return
        
        if frame_original is None:
            if root and not stop_flag.is_set():
                root.after(10, update_frame)
            return
        
        # L·∫•y k√≠ch th∆∞·ªõc frame t·ª´ frame_original (sau khi ƒë√£ check None)
        try:
            frame_w, frame_h = frame_original.shape[1], frame_original.shape[0]
        except (AttributeError, IndexError) as e:
            print(f"‚ö† Error getting frame dimensions: {e}")
            if root and not stop_flag.is_set():
                root.after(10, update_frame)
            return
        
        # L∆∞u frame hi·ªán t·∫°i ƒë·ªÉ c√≥ th·ªÉ ch·ª•p ·∫£nh ng·∫´u nhi√™n (thread-safe)
        with current_frame_lock:
            current_frame = frame_original.copy()
        
        # Check detection_queue non-blocking ƒë·ªÉ l·∫•y hand landmarks m·ªõi nh·∫•t
        result = None
        inference_time = 0
        inference_end_time = None
        
        try:
            detection_data = detection_queue.get_nowait()
            frame_id_det, result, inference_time, inference_end_time = detection_data
            detection_queue.task_done()
            with latest_detection_lock:
                latest_detection = (result, inference_time, inference_end_time)
            # Ki·ªÉm tra inference_time > 0 tr∆∞·ªõc khi t√≠nh reciprocal (tr√°nh ZeroDivision)
            if inference_time > 0:
                inference_fps_list.append(1.0 / inference_time)
                inference_fps_list = limit_list_size(inference_fps_list, MAX_FPS_HISTORY)
                inference_times.append(inference_time)
                inference_times = limit_list_size(inference_times, MAX_FPS_HISTORY)
        except Empty:
            with latest_detection_lock:
                if latest_detection is not None:
                    result, inference_time, inference_end_time = latest_detection
        
        # ƒêo Input FPS th·ª±c t·∫ø
        if prev_capture_time is not None:
            capture_interval = frame_time - prev_capture_time
            if capture_interval > 0:
                input_fps_list.append(1.0 / capture_interval)
                input_fps_list = limit_list_size(input_fps_list, MAX_FPS_HISTORY)
        prev_capture_time = frame_time
        
        # T√≠nh latency (ch·ªâ khi c√≥ inference_end_time h·ª£p l·ªá)
        current_display_time = time.time()
        if inference_end_time is not None:
            display_latency = current_display_time - inference_end_time
            display_latencies.append(display_latency)
            display_latencies = limit_list_size(display_latencies, MAX_FPS_HISTORY)
        else:
            display_latency = 0  # Ch∆∞a c√≥ detection n√†o
        
        # T√≠nh frame interval
        if frame_count == 0:
            frame_interval = 0
            prev_display_time = current_display_time
        else:
            frame_interval = current_display_time - prev_display_time
            prev_display_time = current_display_time
        
        if frame_interval > 0:
            frame_intervals.append(frame_interval)
            frame_intervals = limit_list_size(frame_intervals, MAX_FPS_HISTORY)
        
        frame_count += 1
        
        # S·ªë b√†n tay (d·ª±a tr√™n MediaPipe)
        num_objects = 0
        if result and result.hand_landmarks:
            num_objects = len(result.hand_landmarks)
        total_objects += num_objects
        
        # T√≠nh FPS hi·ªÉn th·ªã
        current_fps = None
        if frame_interval > 0:
            current_fps = 1.0 / frame_interval
            fps_list.append(current_fps)
            fps_list = limit_list_size(fps_list, MAX_FPS_HISTORY)
        
        # T√≠nh trung b√¨nh c√°c FPS metrics
        current_inference_fps = 1.0 / inference_time if inference_time > 0 else None
        avg_fps_display = moving_avg(fps_list)
        avg_inference_fps_display = moving_avg(inference_fps_list)
        avg_input_fps_display = moving_avg(input_fps_list)
        avg_display_latency = moving_avg(display_latencies)
        avg_inference_time = moving_avg(inference_times)
        current_input_fps = input_fps_list[-1] if input_fps_list else None
        
        # Visualization (MediaPipe hand landmarks + bounding box)
        annotated_frame = frame_original.copy()
        
        if result and result.hand_landmarks:
            try:
                # Cleanup old EMA state (prevent memory leak)
                current_hand_indices = set(range(len(result.hand_landmarks)))
                cleanup_old_ema_state(current_hand_indices)
                
                for hand_idx, landmarks in enumerate(result.hand_landmarks):
                    # landmarks: list 21 ƒëi·ªÉm, m·ªói ƒëi·ªÉm c√≥ x, y (normalized)
                    # Validate v√† clamp x, y trong kho·∫£ng [0, 1] ƒë·ªÉ tr√°nh crash n·∫øu MediaPipe tr·∫£ v·ªÅ gi√° tr·ªã l·ªói
                    landmarks_array = np.array([
                        [max(0.0, min(1.0, lm.x)) * frame_w, max(0.0, min(1.0, lm.y)) * frame_h, 1.0] 
                        for lm in landmarks
                    ], dtype=np.float32)
                    
                    # Apply EMA smoothing to reduce jitter
                    landmarks_array = apply_ema_smoothing(hand_idx, landmarks_array, alpha=EMA_ALPHA)
                    
                    xs = landmarks_array[:, 0]
                    ys = landmarks_array[:, 1]

                    # Bounding box theo keypoints
                    min_x, max_x = int(xs.min()), int(xs.max())
                    min_y, max_y = int(ys.min()), int(ys.max())
                
                    # L·ªçc theo k√≠ch th∆∞·ªõc box
                    box_w = max_x - min_x
                    box_h = max_y - min_y
                    if box_w <= 0 or box_h <= 0:
                        continue
                    box_area = box_w * box_h
                    frame_area = float(frame_w * frame_h)
                    area_ratio = box_area / frame_area if frame_area > 0 else 0.0

                    # B·ªè box qu√° nh·ªè (nhi·ªÖu) ho·∫∑c qu√° l·ªõn (th∆∞·ªùng l√† g·∫ßn camera)
                    if area_ratio < HAND_MIN_AREA_RATIO or area_ratio > HAND_MAX_AREA_RATIO:
                        continue

                    # L·ªçc th√™m theo ƒë·ªô tin c·∫≠y handedness ƒë·ªÉ tr√°nh patch m·ªù m·ªù b·ªã g√°n tay
                    handedness_label = "Hand"
                    handedness_score = 1.0
                    if result.handedness and len(result.handedness) > hand_idx:
                        entry = result.handedness[hand_idx]
                        # X·ª≠ l√Ω an to√†n: entry c√≥ th·ªÉ l√† list/tuple ho·∫∑c object tr·ª±c ti·∫øp
                        if isinstance(entry, (list, tuple)) and len(entry) > 0:
                            cat = entry[0]
                        else:
                            cat = entry
                        
                        # ƒê·ªçc category_name v√† score (h·ªó tr·ª£ nhi·ªÅu version MediaPipe)
                        name = getattr(cat, "category_name", None) or getattr(cat, "label", None) or "Hand"
                        score = getattr(cat, "score", None) or getattr(cat, "confidence", None) or 1.0
                        handedness_label = f"{name}:{float(score):.2f}"
                        handedness_score = float(score)
                    # N·∫øu ƒë·ªô tin c·∫≠y handedness qu√° th·∫•p th√¨ b·ªè qua (kh√¥ng v·∫Ω tay)
                    # Lo·∫°i b·ªè c√°c detection kh√¥ng ch·∫Øc ch·∫Øn (c√≥ th·ªÉ l√† false positive)
                    if handedness_score < HANDEDNESS_SCORE_THRESHOLD:
                        continue

                    color = get_track_color(hand_idx)  # d√πng index tay l√†m ID t·∫°m

                    label = f"ID:{hand_idx} {handedness_label}"
                    
                    # V·∫Ω bounding box
                    cv2.rectangle(annotated_frame, (min_x, min_y), (max_x, max_y), color, 2)
                    (text_width, text_height), baseline = cv2.getTextSize(
                        label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1
                    )
                    # ƒê·∫£m b·∫£o text box kh√¥ng v·∫Ω ra ngo√†i frame (y >= 0)
                    text_y_start = max(0, min_y - text_height - baseline - 3)
                    text_y_end = min_y
                    cv2.rectangle(
                        annotated_frame,
                        (min_x, text_y_start),
                        (min_x + text_width, text_y_end),
                        color,
                        -1,
                    )
                    # ƒê·∫£m b·∫£o text lu√¥n visible (tr√°nh edge case khi min_y r·∫•t nh·ªè)
                    text_y = max(text_height + baseline + 2, min_y - baseline - 1)
                    white = (255, 255, 255)
                    cv2.putText(
                        annotated_frame,
                        label,
                        (min_x, text_y),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        0.5,
                        white,
                        1,
                    )
                    
                    # Skeleton + keypoints
                    draw_hand_skeleton(annotated_frame, landmarks_array, color, 1, conf_threshold=0.0)
                    draw_keypoints(annotated_frame, landmarks_array, color, 3, conf_threshold=0.0)
                    
                    # T·ª± ƒë·ªông l∆∞u ·∫£nh b√†n tay (raw data - kh√¥ng c√≥ keypoints)
                    if SAVE_HAND_IMAGES:
                        save_hand_image(frame_original, min_x, min_y, max_x, max_y)

            except Exception as e:
                print(f"‚ö† Error drawing MediaPipe results: {e}")
        
        # Hi·ªÉn th·ªã v·ªõi Tkinter
        try:
            # Convert BGR to RGB for PIL
            rgb_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)
            
            # Resize ƒë·ªÉ fill video container (scale v√† crop center ƒë·ªÉ kh√¥ng c√≥ m√†u ƒëen)
            h, w = rgb_frame.shape[:2]
            try:
                # L·∫•y k√≠ch th∆∞·ªõc video container t·ª´ cache (tr√°nh g·ªçi winfo m·ªói frame)
                container_w = cached_container_size.get('w', WINDOW_WIDTH)
                container_h = cached_container_size.get('h', WINDOW_HEIGHT)
                
                # N·∫øu container ch∆∞a ƒë∆∞·ª£c render, d√πng default size
                if container_w <= 1 or container_h <= 1:
                    container_w = WINDOW_WIDTH
                    container_h = WINDOW_HEIGHT
                
                # Cache resize parameters ƒë·ªÉ tr√°nh t√≠nh l·∫°i m·ªói frame
                cache_key = f"{w}_{h}_{container_w}_{container_h}"
                if ('last_resize_key' not in cached_container_size or 
                    cached_container_size['last_resize_key'] != cache_key):
                    # T√≠nh l·∫°i resize parameters khi size thay ƒë·ªïi
                    scale_w = container_w / w
                    scale_h = container_h / h
                    scale = max(scale_w, scale_h)
                    
                    new_w = int(w * scale)
                    new_h = int(h * scale)
                    
                    # Cache l·∫°i ƒë·ªÉ d√πng cho frame ti·∫øp theo
                    cached_container_size['last_resize_key'] = cache_key
                    cached_container_size['cached_scale'] = scale
                    cached_container_size['cached_new_w'] = new_w
                    cached_container_size['cached_new_h'] = new_h
                else:
                    # D√πng l·∫°i cached values khi size kh√¥ng ƒë·ªïi
                    scale = cached_container_size['cached_scale']
                    new_w = cached_container_size['cached_new_w']
                    new_h = cached_container_size['cached_new_h']
                
                # Resize v·ªõi cached parameters
                if abs(scale - 1.0) > 0.01:
                    interpolation = cv2.INTER_AREA if scale < 1.0 else cv2.INTER_LINEAR
                    rgb_frame = cv2.resize(rgb_frame, (new_w, new_h), interpolation=interpolation)
                
                # Crop center ƒë·ªÉ fit container (v·ªõi validation ƒë·ªÉ tr√°nh crash)
                if new_w > container_w or new_h > container_h:
                    start_x = max(0, min((new_w - container_w) // 2, new_w - container_w))
                    start_y = max(0, min((new_h - container_h) // 2, new_h - container_h))
                    end_x = min(new_w, start_x + container_w)
                    end_y = min(new_h, start_y + container_h)
                    rgb_frame = rgb_frame[start_y:end_y, start_x:end_x]
                elif new_w < container_w or new_h < container_h:
                    # Pad v·ªõi m√†u ƒëen n·∫øu nh·ªè h∆°n container (√≠t khi x·∫£y ra)
                    pad_w = (container_w - new_w) // 2
                    pad_h = (container_h - new_h) // 2
                    rgb_frame = cv2.copyMakeBorder(
                        rgb_frame, pad_h, container_h - new_h - pad_h,
                        pad_w, container_w - new_w - pad_w,
                        cv2.BORDER_CONSTANT, value=[0, 0, 0]
                    )
            except Exception:
                # Fallback: gi·ªØ nguy√™n k√≠ch th∆∞·ªõc
                pass
            
            # Reuse PhotoImage object ƒë·ªÉ t·ªëi ∆∞u performance (PIL.ImageTk.PhotoImage c√≥ paste() method)
            pil_image = Image.fromarray(rgb_frame)
            
            if not hasattr(video_label, 'photo_image') or video_label.photo_image is None:
                # L·∫ßn ƒë·∫ßu: t·∫°o m·ªõi PhotoImage
                video_label.photo_image = ImageTk.PhotoImage(image=pil_image)
                video_label.photo_image_size = pil_image.size
                video_label.config(image=video_label.photo_image, text="")
            else:
                # Update PhotoImage hi·ªán c√≥ n·∫øu size gi·ªëng nhau (d√πng paste() - nhanh h∆°n)
                try:
                    if hasattr(video_label, 'photo_image_size') and pil_image.size == video_label.photo_image_size:
                        # D√πng paste() ƒë·ªÉ update image (t·ª± ƒë·ªông reflect, kh√¥ng c·∫ßn recreate)
                        video_label.photo_image.paste(pil_image)
                    else:
                        # T·∫°o m·ªõi n·∫øu size thay ƒë·ªïi
                        video_label.photo_image = ImageTk.PhotoImage(image=pil_image)
                        video_label.photo_image_size = pil_image.size
                        video_label.config(image=video_label.photo_image, text="")
                except Exception:
                    # Fallback: t·∫°o m·ªõi PhotoImage n·∫øu c√≥ l·ªói
                    video_label.photo_image = ImageTk.PhotoImage(image=pil_image)
                    video_label.photo_image_size = pil_image.size
                    video_label.config(image=video_label.photo_image, text="")
            
            # Update status (kh√¥ng override n·∫øu ƒëang pause)
            if status_label and not is_paused:
                if current_fps is not None and avg_fps_display is not None:
                    status_label.config(text="‚óè Running", fg='#00ff00')
            
            # Update metrics labels (ch·ªâ update khi gi√° tr·ªã thay ƒë·ªïi)
            if metrics_labels:
                if current_fps is not None and avg_fps_display is not None:
                    # T√≠nh to√°n c√°c gi√° tr·ªã m·ªõi
                    new_values = {
                        'target_fps': f"{target_fps:.1f}",
                        'display_fps': fps_text(current_fps, avg_fps_display),
                        'inference_fps': fps_text(current_inference_fps, avg_inference_fps_display) if current_inference_fps else '--',
                        'input_fps': fps_text(current_input_fps, avg_input_fps_display) if current_input_fps else '--',
                        'latency': ms_text(display_latency*1000, avg_display_latency*1000 if avg_display_latency else None),
                        'inference_time': ms_text(inference_time*1000, avg_inference_time*1000 if avg_inference_time else None),
                        'objects': f"{num_objects}"
                    }
                    
                    # Ch·ªâ update labels khi gi√° tr·ªã thay ƒë·ªïi
                    for key, new_value in new_values.items():
                        if key not in cached_metrics_values or cached_metrics_values[key] != new_value:
                            metrics_labels[key].config(text=new_value)
                            cached_metrics_values[key] = new_value
                else:
                    # Ch∆∞a c√≥ FPS data (ch∆∞a kh·ªüi ƒë·ªông xong)
                    init_values = {
                        'target_fps': f"{target_fps:.1f}",
                        'display_fps': '--',
                        'inference_fps': '--',
                        'input_fps': '--',
                        'latency': '--',
                        'inference_time': '--',
                        'objects': f"{num_objects}"
                    }
                    
                    # Ch·ªâ update labels khi gi√° tr·ªã thay ƒë·ªïi
                    for key, new_value in init_values.items():
                        if key not in cached_metrics_values or cached_metrics_values[key] != new_value:
                            metrics_labels[key].config(text=new_value)
                            cached_metrics_values[key] = new_value
        except Exception as e:
            print(f"‚ö† Error updating Tkinter UI: {e}")
        
        # Print info (th·ªëng k√™ FPS / latency)
        if frame_count % PRINT_EVERY_N_FRAMES == 0 or frame_count <= 5:
            if len(fps_list) > 0:
                avg_frame_interval = (moving_avg(frame_intervals) or 0) * 1000
                avg_display_latency = (moving_avg(display_latencies) or 0) * 1000
                avg_fps_print = avg_fps_display or moving_avg(fps_list) or 0
                avg_inference_fps_print = moving_avg(inference_fps_list) or 0
                avg_input_fps_print = moving_avg(input_fps_list) or 0
                print(
                    f"  ‚Üí Average Display FPS: {avg_fps_print:.1f} | "
                    f"Average MediaPipe FPS: {avg_inference_fps_print:.1f} | "
                    f"Average Input FPS: {avg_input_fps_print:.1f} | "
                    f"Target FPS: {target_fps:.1f} | "
                    f"Frame interval: {avg_frame_interval:.1f}ms | "
                    f"Display latency: {avg_display_latency:.1f}ms | "
                    f"Inference: {inference_time*1000:.1f}ms"
                )
        
        # Schedule next update
        if not stop_flag.is_set():
            delay = 10  # 10ms delay
            root.after(delay, update_frame)
        
    except Exception as e:
        print(f"‚úó Error in update_frame: {e}")
        if not stop_flag.is_set():
            root.after(10, update_frame)

# Ch·∫°y Tkinter main loop
root.after(10, update_frame)
root.mainloop()

# ---------- 5. Cleanup & Summary ----------
# D·ª´ng t·∫•t c·∫£ threads
stop_flag.set()

# Queue cleanup: d√πng get_nowait() v·ªõi Empty exception
try:
    while True:
        try:
            frame_queue.get_nowait()
            frame_queue.task_done()
        except Empty:
            break
    while True:
        try:
            display_frame_queue.get_nowait()
            display_frame_queue.task_done()
        except Empty:
            break
    while True:
        try:
            detection_queue.get_nowait()
            detection_queue.task_done()
        except Empty:
            break
except Exception:
    pass

# ƒê·ª£i threads k·∫øt th√∫c ho√†n to√†n
if thread1.is_alive():
    thread1.join(timeout=3)
if thread2.is_alive():
    thread2.join(timeout=3)

# ƒê√≥ng landmarker ƒë·ªÉ gi·∫£i ph√≥ng t√†i nguy√™n
try:
    landmarker.close()
except Exception:
    pass

# Cleanup Tkinter (n·∫øu ch∆∞a ƒë∆∞·ª£c destroy)
try:
    if root.winfo_exists():
        root.quit()
        root.destroy()
except Exception:
    pass

pred_end = time.time()
pred_time = pred_end - pred_start

# T√≠nh to√°n th·ªëng k√™ cu·ªëi c√πng
avg_fps = sum(fps_list) / len(fps_list) if fps_list else 0
avg_frame_interval = sum(frame_intervals) / len(frame_intervals) * 1000 if frame_intervals else 0
avg_display_latency = sum(display_latencies) / len(display_latencies) * 1000 if display_latencies else 0
min_display_latency = min(display_latencies) * 1000 if display_latencies else 0
max_display_latency = max(display_latencies) * 1000 if display_latencies else 0
avg_inference_fps = sum(inference_fps_list) / len(inference_fps_list) if inference_fps_list else 0
avg_input_fps = sum(input_fps_list) / len(input_fps_list) if input_fps_list else 0

total_end = time.time()

print(f"\n{'='*60}")
print(f"REALTIME SUMMARY - MEDIAPIPE HAND LANDMARKER:")
print(f"  Backend: MediaPipe (hand_landmarker.task)")
print(f"  Total frames processed: {frame_count}")
print(f"  Total objects detected: {total_objects}")
print(f"  Target FPS: {target_fps:.1f}")
print(f"  Average Display FPS: {avg_fps:.2f}")
print(f"  Average MediaPipe FPS: {avg_inference_fps:.2f}")
print(f"  Average Input FPS: {avg_input_fps:.2f}")
print(f"  Average frame interval: {avg_frame_interval:.2f}ms")
print(f"  Average display latency: {avg_display_latency:.2f}ms")
print(f"  Min display latency: {min_display_latency:.2f}ms | Max display latency: {max_display_latency:.2f}ms")
print(f"  Total inference time: {pred_time:.2f}s")
print(f"  Total script time: {total_end - total_start:.2f} seconds")
with queue_drop_lock:
    print(f"  Queue drops (frames): {queue_drop_count}")
if avg_fps > 0:
    efficiency = (avg_fps / target_fps) * 100
    print(f"  Efficiency: {efficiency:.1f}% (vs target FPS)")